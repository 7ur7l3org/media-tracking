"media tracker" is what the open source community calls this stuff

vcs/files/mirroring, single web dashboard with efficient search, offline, tracking things not in any database (yet?) like youtube stuff and genshin stuff

watchlist(s) with upvoting per entry
watch history easy to find and filter and see from the main page, multiple watches easily visible

directly links to all main database sources, e.g. https://yamtrack.fuzzygrim.com/details/tmdb/movie/38/eternal-sunshine-of-the-spotless-mind does that well. yamtrack is also just fast and pretty

https://github.com/bonukai/MediaTracker is a good starting point..

yamtrack has the best url api and has movies/anime/tv, but terrible actual tracking
mediatracker has better actual tracking and is simpler
yamtrack doesn't have unified search
yamtrack doesn't have history and stats are shoddy


music/books/games are kinda future stuff.. e.g. maloja/pano-scrobbler is future integration
    https://github.com/krateng/maloja is great for music tracking and https://github.com/kawaiiDango/pano-scrobbler scrobbles to it. probably the best ui in general but unclear where metadata is sourced

databases:
- mal, tvdb, tmdb/omdb/etc, good reads, yt music, wikipedia, and reverse links via tampermonkey for all those sites to this tracker

openlibrary does not show series information..

out-links to streaming sites e.g. private description custom metadata information
or integration with *arr and stremio and such.. providers..

set up jellyseerr and such
https://github.com/Radarr/Radarr
https://old.reddit.com/r/selfhosted/comments/18c0vqr/dont_fully_understand_use_of_radarr/
https://wiki.servarr.com/
prowlarr manages torrent trackers


random gpt suggestions. mostly can find anything from just a https://www.wikidata.org/wiki/Wikidata:Main_Page search
Movies	Wikidata QID	TMDb ID → IMDb ID → TVDb ID
TV Shows	Wikidata QID	TVDb ID → TMDb ID → IMDb ID
Anime	Wikidata QID	AniDB ID → MyAnimeList ID → AniList ID
Books	Wikidata QID	Open Library ID → WorldCat ID → Goodreads ID → ISBN .. or honestly annas archive md5 lol or whatever keys their metadata
Music	Wikidata QID	MusicBrainz ID → Discogs ID
Video Games	Wikidata QID	GiantBomb ID → IGDB ID → Steam App ID

just link to all authoritative sources somewhere to find stuff if wikidata fails

https://sqid.toolforge.org/#/view?id=Q11678&lang=en sqid is sick



okay so idea is more or less wikidata etc search frontend over a few autocommitted (use offline git fs thing for mobile) json files, watch history by id/time where future id's are a superset of all past watch history, and watchlist votes by id/time where future entries are a superset of all watchlist votes. maybe time to learn graphql? when offline or in general allows generic keys to be used temporarily or permanently and then updated with the actual more authoritative links when there's a connection. probably easiest is to enter a url and have it auto-suggest the keys
actually i think i want just all watch history and all watchlist vote history in files that point only to the key, and then entries for keys are backreferenced all watches or watchlist votes alongside notes and added links and such, so everything is always fully connected. add everything ever viewed too and backreferenced to a viewed list json
call it consumed or queued/enlisted instead of watched/watchlist to be more generic to all media
make it capture ctrl+v on the page to take you to the page for that content, or on /<url> that can pick it up by that url so you just put t.7ur.it/<blah> in front of whatever
this might be possible to be ran entirely client side... esp if it has proper git view of the data backend or otherwise filesystem access to the repo. that would be super ideal cause then i could just put it on 7ur.it without hosting anything lol. maybe auth like organice

---

<future>

also make it archive/cache the dynamicially generated htmls with all the information and just update whenever it visits and re-renders. just for posterity would be interesting how much wikidata information is updated

then integrate subtitle databases and stremio search stuff on top of custom descriptions for links temporarily. tho possibly autogen search links for thepiratebay etc with the multiple names reported by wikidata idk, there's probably a solution for that already though. tho also just links to streaming stuff maintained would be nice too. some kinda prototype page info data thing that is always there

then set up or integrate maloja/pano-scrobbler for music too to start capturing that data, esp with lyrics database for search

then make it have links to spin up a web sync play thing for a stream in one click :)

then make it also pull youtube and tick tock watch history and saves...

then make it integrate with *arr and trackers

then add custom bookmarks/wiki pages for stuff like anime summer 2025 and such maybe

also link to script that imports from mal to wikidata for if someone wants to set up a bot to do that in an automated way instead of manually on demand

- long term make qr code to share pat between browser/device indexeddb stores.. or consider branch names gen'd from pat and pat per device. but def make easily copy pastable between places

<now>

thanks https://chatgpt.com/c/679d3064-16b0-800c-9e35-89509164697b (trying o3-mini)
thanks https://chatgpt.com/c/679d8f5e-ad78-800c-ac21-cba9c0ffbd01 (o3-mini is good!)

first make website that links to wikidata and can pull wikidata given wikidata url/id
then make it render all the id links of the content and maybe preview the images and save to media
then make it able to work with the four json files of consumed/enlisted/media/viewed/searched
then make it work with isomorphic-git
then make it work by id or url of external stuff
then make it import from trakt or do that manually idk and see what is missing and was or wasn't able to link

cline with deepseek?


-- make the search be above the fetch and include buttons for each search result to do the fetch for them directly
-- at the top of the fetch data let's include all the Identifiers with each of the IDs and links to each of them, or at least comprehensively display everything that https://www.wikidata.org/wiki/Q116032682 would. maybe also log the sparql and response to the javascript console too so it's more clear exactly what's happening.
-- property descriptions
-- make url query parameters to auto-search a term or auto-fetch by qid on page load -> ?q= or ?id=
-- separate out css and make everything font monospace
-- instead of hardcoding any random identifier mappings, we should automatically use the `formatter URL (P1630)` of any property that has one to hyperlink the value to the url that is constructed from the value
-- make property values such as `	{"entity-type":"item","numeric-id":18665344,"id":"Q18665344"}` human readable
-- make ID in Property (ID) hyperlink to the wikidata page for that property e.g. `instance of (P31)` should make P31 hyperlink to `https://www.wikidata.org/wiki/Property:P31`
-- make value show the human readable version of it when it is a wikidata thing. e.g. `instance of (P31):	Q11424` has Q11424 hyperlinked to https://www.wikidata.org/wiki/Q11424 which shows `film (Q11424) sequence of images that give the impression of movement, stored on film stock` which would be good to display
-- render properties for relevant groups of interest, e.g. group `Wikidata property for items about films (Q22965162)` first, `Wikidata property to identify films (Q29542094)` second, and all other misc properties after that. the property groups should have headers indicating what they are
-- remove full json response from page
-- hitting enter when on the search or fetch info boxes should submit the button query
-- revisit using ordinals for sorting instead of the complicated algorithm probably
-- sequencing information issues:
    - on a movie, works great
    - on the movie film series, doesn't render any information even though e.g. `X (Q114451788)` is a `film series (Q24856)`. we should be able to grab all the `part of the series (P179)` parts of that series. this is working when on a movie in the series, but doesn't work when on the series itself for some reason
    - on a tv series, works mostly great, but the sorting of the collapsed episodes in e.g. `Episodes grouped by season: Season: House, season 1 (Q166042)` don't properly topologically sort. it shows `Paternity (Q2666662): episode of House (S1 E2)` first and `Pilot (Q615483): episode of House` third from last even though the pilot is followed by Paternity and Paternity follows Pilot
    - on a tv season (or on an episode), sees that it's `Series: House (Q23558) (part of series, P179)` but then renders `Parts of this series (via P179)` all episodes and seasons all together in one big list instead of properly rendering like it does when we're on the tv series itself. all series rendering should render equivalently no matter which element of the sequencing tree we are on. also the `Direct sequencing statements` don't get rendered for the seasons even though they have e.g. `part of the series: House (series ordinal 1, follows no value, followed by House, season 2)`
    - also i notice there are `series ordinal (P1545)`s for each of the `has part(s) (P527)` of the seasons, so we can probably directly render and use those for sorting if they exist (otherwise falling back to topological sorting is probably fine)
-- we stopped loading styles.css at some point
-- the property value names broke in a lot of cases for some reason e.g. it renders `Q6371282 (Q6371282)` instead of `House (TV series) (Q6371282)`
-- need to make the search paginated so we can search more results or next/previous 7 etc. should probably search 10 results at a time for now
-- should display (loading) or something in the Fetch results section if it is currently loading or doing the fetch
-- should make the entire search section collapsable and default to collapsed unless we aren't doing a fetch (like ?id= isn't triggering a direct fetch) and the search is the first thing we are doing. should collapse when fetching
-- we should also put the title and qid in the page title whenever we fetch so that the navigation history shows what pages we were on
-- when we update the url we aren't editing the navigation history and we should be making a new navigation history entry every time we write the url so that e.g. back properly goes back
-- remove the `Fetch by Wikidata QID` and `Fetch Info` search/button. the e.g. `Pearl (Q111669794)` and `2022 film directed by Ti West` should be the primary sectioning on the page and if you have a qid you can always plug it into the ?id in the url. also we should delete the `Identifiers` header since it doesn't really add any value
-- we should make the entire series/sequencing information parts section be collapsible (but default to not collapsed always)
-- improve caching of requests because we care way more about responsiveness than live changing data. cache of all wikidata requests should probably live at least 30 mins
-- there are tons of exceptions in the console (probably fixed by caching)
-- fix broken links to properties on wikidata. we are currently linking to e.g. https://www.wikidata.org/wiki/Property/P4908 which should be https://www.wikidata.org/wiki/Property:P4908
-- should link to [sqid] in addition to [wikidata] and should link to ourselves via ?id= as the primary link for all wikidata links
-- Property (ID) keys should be wider in the property table
-- should make the search input/button be in the collapsible search header so that you don't have to uncollapse just to do a search, and doing a search should automatically expand the search section
-- should remove the Fetch button from the search results because the results already hyperlink to ?id=<result id> which is equivalent to what the button does
-- bold or otherwise emphasize all search results that "is a" creative work (Q17537576) as that is mostly what we are looking for
-- change `Series: American Dad! (Q210311) [sqid] [wikidata] [P179 [sqid] [wikidata]]` to be `Series (P179 [sqid] [wikidata]): American Dad! (Q210311) [sqid] [wikidata]`
-- fix rendering of e.g. `{"amount":"+8","unit":"1"}`, `{"text":"House","language":"en"}, {"text":"House, M.D.","language":"en"}`, durations `{"amount":"+43","unit":"http://www.wikidata.org/entity/Q7727"}`, times `{"time":"+2010-07-25T00:00:00Z","timezone":0,"before":0,"after":0,"precision":11,"calendarmodel":"http://www.wikidata.org/entity/Q1985727"}, {"time":"+2011-07-24T00:00:00Z","timezone":0,"before":0,"after":0,"precision":11,"calendarmodel":"http://www.wikidata.org/entity/Q1985727"}`
-- refactor into files that are more maintainable separation of concerns so that we can update smaller files instead of updating one huge file
-- we should e.g. expand season 2 if we're fetching the information for season 2 in the series/sequencing information parts tree. currently it underlines itself in the tree but doesn't expand it to show the episodes in it
-- there are some `series of creative works (Q7725310)` which give `No parts found for this series.` but we can pull everything that is `part of the series (P179)` part of that series, and those things do still have their series ordinal. e.g. a missing `has part(s) (P527)` forward reference should not imply the lack of existing series/ordinal back-linkages <- file:///C:/Users/7UR7L3/dev/media-tracking/site/index.html?id=Q190525 demonstrates fix
-- make sure the series/sequencing display can handle when something is a part of multiple series, it should render each series that it's a part of. e.g. file:///C:/Users/7UR7L3/dev/media-tracking/site/index.html?id=Q155653 is only showing one right now
-- currently things that aren't series and aren't part of any series display as Series: <themselves> and No parts found for this series. really it should just omit the Series/Sequencing Information when not applicable. i wonder if the fallback is to itself if it isn't part of a series and isn't "is a" series itself
-- update series/sequencing css to align the bullets with the collapsible sections. currently the collapsible sections are double indented and not in line with the other adjacent bullets that don't have sub-parts
-- interesting duplication issue in the recursive parts series/sequencing view. not sure what to do about this lol (file:///C:/Users/7UR7L3/dev/media-tracking/site/index.html?id=Q169042) <- i ingore it is what i do
-- expand/collapse all buttons should exist for the properties and for the series/sequencing information parts
-- consider compressing json? no because git already does blobs? e.g. fetch will be small even though repo is decently big in indexeddb? yeah probably. assuming i have the storage for like 10MB which surely i do. yeah actually my architecture is cracked


- similarly to how we derive series information from `part of the series (P179)` backreferences for all things that are part of that series, we should be able to go to a person and pull all entities where that person is the director or cast member or screenwriter or producer or etc. etc. any other "wikidata property related to creative works" of that thing, like we should be able to pull all the things that the guy was a director of, or all the things that a guy was a cast member in, etc. similarly we should be able to pull all works where the main subject is xyz too, or where the genre is xyz. though i guess that might get pretty enormous so probably we'd focus on people first. for now the strat is to just go to sqid to find the related entities

- series/sequencing information things:
    - somehow make copy paste on the series/sequencing information section retain the section/subsection indentation
    - somehow make copy paste copy hyperlinks too in addition to text content

- prioritize some properties over others
- let's load some images!
- add filters to the search to only search for thinigs that are e.g. films
- include title in url that is ignored, maybe in the # or something that indicates metadata that isn't important to the url
- make caching longer, like 6h maybe
- consolidate all ({qid}[sqid][wikidata]) rendering logic to a single place, and have localStorage setting to be able to toggle id rendering entirely on and off
- navigating back to a previously visited page shouldn't have to load and perform a search at all if it's fully rendered.. only a refresh should trigger that
- make https://www.wikidata.org/wiki/Q63348493 show up bold in the search. e.g. anime television series is yes. also television program for https://www.wikidata.org/wiki/Q28856068
- fix wolfram language entity code hyperlinks e.g. file:///C:/Users/7UR7L3/dev/media-tracking/site/index.html?id=Q788822 has a broken link where it doesn't cram the whole value in for some reason
- clear cache button or manually re-fetch-data button for when i update wikidata but my cache is now bad
- prioritize search results for bolds such that e.g. first half are all bold even if it has to pre-search multiple pages for them. maybe. idk
- make wikidata updater suggest ensuring e.g. (S3E13) is in the description of the entity
- make site happy with chrome url autocomplete however that works, like i should type track.7<tab>the wild robot and it should go to `track.7ur.it?q=the wild robot`
- make times be mountain time in json? probably. user settings? idk lol i guess settings in the repo seems kinda reasonable tho
- update with new trakt export at migration time, for now tracking things in trakt
- make search have "search by" to search by trakt/imdb/tvdb/letterboxed/mal url or id

everything chips

failure cache
create tool
google authority?
torrent wikidata? sparql annas archive?
let him crockpot
snapshot version of entity and render on page
get all instances of Wikidata property to identify films (Q29542094) to automate the creation of new wikidata entries

on load of a large series (that maybe doesn't hit the cache?), yet everything seems to work fine so not sure what these requests are on about:
Access to fetch at 'https://query.wikidata.org/sparql?query=%0A%20%20%20%20%20%20SELECT%20%3Fpart%20%3FpartLabel%20%3FpartDescription%20%3Fordinal%20WHERE%20%7B%0A%20%20%20%20%20%20%20%20wd%3AQ4854278%20p%3AP527%20%3Fstmt.%0A%20%20%20%20%20%20%20%20%3Fstmt%20ps%3AP527%20%3Fpart.%0A%20%20%20%20%20%20%20%20OPTIONAL%20%7B%20%3Fstmt%20pq%3AP1545%20%3Fordinal.%20%7D%0A%20%20%20%20%20%20%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22.%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20ORDER%20BY%20%3Fordinal%0A%20%20%20%20&format=json' from origin 'null' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.
GET https://query.wikidata.org/sparql?query=%0A%20%20%20%20%20%20SELECT%20%3Fpart%20%3FpartLabel%20%3FpartDescription%20%3Fordinal%20WHERE%20%7B%0A%20%20%20%20%20%20%20%20wd%3AQ4854278%20p%3AP527%20%3Fstmt.%0A%20%20%20%20%20%20%20%20%3Fstmt%20ps%3AP527%20%3Fpart.%0A%20%20%20%20%20%20%20%20OPTIONAL%20%7B%20%3Fstmt%20pq%3AP1545%20%3Fordinal.%20%7D%0A%20%20%20%20%20%20%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22.%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20ORDER%20BY%20%3Fordinal%0A%20%20%20%20&format=json net::ERR_FAILED 429 (Too Many Requests)
series.js:69 Error fetching series parts: TypeError: Failed to fetch
    at getRequest.onsuccess (cache.js:61:13)


get openrefine working to create missing entries
make page that encourages getting stuff into wikidata with tools/automation depending on the content type (do movies first)


get actual history/list stuff working
well maybe first see how much coverage we do or don't have tbh
make work with arbitrary id's (normally url's) ?
import trakt and see how far we get




-- noah league
-- avs game
isogit browser cli pwa with persistent storage. how to auth? vps for isomorphic-git proxy cors bs? in-page cli/terminal?
integrate git into content tracker
single json should be keyed by qid/url/id and hold all history and all queue votes and all notes. it should be trivial to render all watches in order of time, or render all queue in order of number of votes or last vote time. cause surely only like 100k or so entries would ever exist



make trakt -> media tracker json

sort (and merge dupe records?) media-tracking.json . why sort? idk.

make tracker load json (manually from file for now)

make series sequencing media pages show all queue votes and all consumptions
^^^^^^^^^^^^^^^^^^

get tracker showing /history and /queue and content pages showing all consumptions and queue votes. top links [media] page and [queues] with the watchlist queue and [consumption] where [consumption] shows by all chronologically (optionally filtering by wikidata "is a" content type), or by progress by shows in order of progress to completion and then completion (do i need to save off all series/sequencing information to a file? damn maybe.. probably.. actually probably unnecessary cause just need the sequencing information which is calc'd once and cached for all shows you've watched and that's only like a few thousand maybe)

make tracker commit history records or queue-votes to the file with in-browser git
make git auth and push to remote proxy per-user somehow (buy a vps finally?)

make thing to make wikidata stubs for anime and for anime seasons and for anime episodes for mal hits, should send you to wikidata with the query to run or whatever so anyone can run the command directly. sub-search will search on tmdb/mal and give links to add to wikidata or will show existing wikidata vs what needs to be created. should also check and make stubs for new episodes that recently came out that wouldn't be on wikidata yet. maybe a button to consult e.g. mal/tmdb on the page which will compare wikidata with that and prompt to add to wikidata including adding missing authority references (e.g. weird non-canonical trakt slug of file:///C:/Users/7UR7L3/dev/media-tracking/site/index.html?id=Q130353420) and description naming info etc to wikidata. should take you to a quickstatements to run

https://quickstatements.toolforge.org/#/batch/185948
maybe figure out how to make this kind of thing https://www.wikidata.org/w/index.php?title=Q109526557&action=history&offset=&limit=50 referenced in e.g. `10:52, 4 May 2023 CennoxX talk contribs  38,621 bytes +850  Added reference to claim: Trakt.tv ID (P8013): shows/one-piece-184618, batch #185948 undothank Tag: quickstatements [2.0] (restore)`, https://quickstatements.toolforge.org/#/batches/CennoxX . that's gotta be the best way to do stuff lol. or at least each of my creation stuff should be a batch like this
in fact it has `based on heuristic [P887]: inferred from IMDb ID database lookup [Q115288501]  retrieved [P813]: 2023-05-04` which i can probably do for my stuff cause that looks hot af
https://www.wikidata.org/wiki/Wikidata:WikiProject_Movies/Properties#Description ooh i think i don't have to provide a description. that would be nice. sounds like director and publication date is desired. publication date probably from tmdb ig. idk tough. https://www.wikidata.org/wiki/Help:Description common formulas too






- push all this to github
- add "login with github" and use https://cors.isomorphic-git.org/ for auth, store oauth token in indexeddb (https://chatgpt.com/c/67b54ff5-8b00-800c-9ce4-f6ae553131b7)
- ensure all content is set via .textContent and not .innerHTML (from wikidata and all other data sources) to protect against xss
- add ui for json modification
    - add generic key/url tracking
- local commit json to branch
- push json to github when online
- figure out how to do local storage pwa on filesystem for offline explicit repo that can be termux managed
- ideally three layers are push to indexeddb, push to local filesystem if granted access, push to github
    - possibly do the latter and if that succeeds try the middle and if that fails do the first
    - always show exact push/sync status

- migrate git pat token auth from github api to isomorphic-git
- make isomorphic-git work with actual filesystem access if it's not annoying to regrant permission constantly